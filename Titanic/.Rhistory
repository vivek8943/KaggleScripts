aggr(train, delimiter = NULL, plot = TRUE)
train
aggr(train, plot = TRUE)
library(VIM)
aggr(train, plot = TRUE)
localH2O <- h2o.init(ip = "localhost", port = 54321,
max_mem_size = '12g', min_mem_size = '4g', nthreads = -1)
library(h2o)
library(h2o)
library(readr)
localH2O <- h2o.init(ip = "localhost", p
ort = 54321,
max_mem_size = '12g', min_mem_size = '4g', nthreads = -1)
localH2O <- h2o.init(ip = "localhost", port = 54321,
max_mem_size = '12g', min_mem_size = '4g', nthreads = -1)
aggr(train, plot = TRUE)
d.train.demo <- read.csv('~/Documents/mvakaggle/prudtrain.csv', stringsAsFactors=F)
aggr(d.train.demo, plot = TRUE)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321, startH2O = TRUE,
Xmx = '2g')
dat_h2o <- h2o.importFile(localH2O, path = "/Users/Vivekanand/Desktop/Titanic/cleanedtrain.csv")
data(package="fma")
require(fma)
require("fma")
install.packages("fma")
library(fma)
data(package="fma")
data("dole")
head(dole)
str(dole)
plot(dole)
plot(dole)
dole.df=as.data.frame(dole)
str(dole.df)
View(dole.df)
View(dole.df)
View(dole.df)
plot(dole.df)
axis(2,at=marks,labels=format(marks,scientific=FALSE))
plot(dole.df,xlab="Year",ylab="Thousands",ylim = c(0, max(dole.df)))
plot(dole.df,xlab="Year",ylab="Total",ylim = c(0, max(dole.df)))
plot(dole.df,xlab="Year",ylab="Total",ylim = c(0, 10000))
plot(dole.df,xlab="Year",ylab="Total")
axis(2,at=dole.df,labels=format(dole.df,scientific=FALSE))
axis(2,at=dole.df$x,labels=format(dole.df$x,scientific=FALSE))
source('~/.active-rstudio-document', echo=TRUE)
library(fma)
data(package="fma")
data("dole")
plot(dole.df,xlab="Year",ylab="Total")
dole.df=as.data.frame(dole)
str(dole.df)
seasonplot(dole.df)
plot(log(dole.df),xlab="Year",ylab="Total")
plot(log2(dole.df),xlab="Year",ylab="Total")
plot(log(dole.df),xlab="Year",ylab="Total")
plot(dole.df,xlab="Year",ylab="Total")
lambda <- BoxCox.lambda(dole.df) # = 0.27
plot(BoxCox(dole.df,lambda))
lambda <- BoxCox.lambda(dole.df$x) # = 0.27
plot(BoxCox(dole.df,lambda))
plot(BoxCox(dole.df$x,lambda))
plot(BoxCox(dole.df,lambda))
plot(log(dole.df$x),xlab="Year",ylab="Total")
plot(log(dole.df),xlab="Year",ylab="Total")
plot(log(dole.df),xlab="Year",ylab="Log transformed Total")
lambda <- BoxCox.lambda(dole.df$x) # = 0.27
lambda <- BoxCox.lambda(dole.df) # = 0.27
lambda <- BoxCox.lambda(dole.df$x)
plot(BoxCox(dole.df,lambda))
plot(BoxCox(dole.df,lambda),xlab="Year",ylab="Box Cox transformed Total")
data("dole")
data(package="fma")
data("usdeaths")
str(usdeaths)
plot(usdeaths,xlab="Year",ylab="Total")
plot(log(usdeaths),xlab="Year",ylab="Log transformed Total")
lambda <- BoxCox.lambda(usdeaths)
plot(BoxCox(usdeaths,lambda),xlab="Year",ylab="Box Cox transformed Total")
plot(usdeaths,xlab="Year",ylab="Total")
data(package="fma")
#bricksq
data("bricksq")
str(bricksq)
plot(bricksq,xlab="Year",ylab="Total")
#Log transformation
plot(log(bricksq),xlab="Year",ylab="Log transformed Total")
#Box Cox transformation
lambda <- BoxCox.lambda(bricksq)
plot(BoxCox(bricksq,lambda),xlab="Year",ylab="Box Cox transformed Total")
plot(bricksq,xlab="Year",ylab="Total")
#Log transformation
plot(log(bricksq),xlab="Year",ylab="Log transformed Total")
#Box Cox transformation
lambda <- BoxCox.lambda(bricksq)
plot(BoxCox(bricksq,lambda),xlab="Year",ylab="Box Cox transformed Total")
plot(sqrt(bricksq),xlab="Year",ylab="Log transformed Total")
plot(log(bricksq),xlab="Year",ylab="Log transformed Total")
scatter.smooth(bricksq)
plot(BoxCox(bricksq,lambda),xlab="Year",ylab="Box Cox transformed Total")
scatter.smooth(bricksq)
qplot(bricksq, geom='smooth', span =0.5)
library(ggplot2)
qplot(bricksq, geom='smooth', span =0.5)
dole.df
typeof(dole.df)
str(dole.df)
plot.ts(log(usdeaths),xlab="Year",ylab="Log transformed Total")
plot.ts(log(dole.df),xlab="Year",ylab="Log transformed Total")
data("dole")
plot(dole.df,xlab="Year",ylab="Total")
dole.df=as.data.frame(dole)
library(fma)
library(ggplot2)
plot.ts(dole.df,xlab="Year",ylab="Total")
plot(dole.df,xlab="Year",ylab="Total")
library(fpp)
install.packages("fpp")
library(fpp)
data(package="fpp")
plot.ts(log(dole.df),xlab="Year",ylab="Log transformed Total")
plot(log(dole.df),xlab="Year",ylab="Log transformed Total")
require("TTR")
install.packages("TTR")
library(TTR)
plot(dole.df,xlab="Year",ylab="Total")
plot(SMA(dole.df,n=3),xlab="Year",ylab="Log transformed Total")
plot.ts(SMA(dole.df,n=3),xlab="Year",ylab="Log transformed Total")
plot(dole.df,xlab="Year",ylab="Total")
plot.ts(log(dole.df),xlab="Year",ylab="Log transformed Total")
comp<-decompose(dole)
comp
qplot(comp$x,d geom='smooth', span =0.5)
qplot(comp$x,geom='smooth', span =0.5)
comp
plot(comp)
comp1<-decompose(log(dole)
plot(comp1)
comp1<-decompose(log(dole)
)
plot(comp1)
plot(comp)
plot(comp1)
plot(originalcomponents)
originalcomponents<-decompose(dole)
plot(originalcomponents)
train <- read.csv("train.csv")
test <- read.csv("test.csv")
library(rpart)
library(party)
library(rattle)
library(rpart)
library(party)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
train$Name[1]
test$Survived <- NA
load("/Users/Vivekanand/Desktop/train_clean.RData")
setwd("~/Desktop/Titanic")
setwd("~/Desktop/Titanic")
train<-read.csv("train.csv",na.strings=c('NA',''),stringsAsFactors=F)
test<-read.csv("test.csv",na.strings=c('NA',''),stringsAsFactors=F)
#checking the missing data
#library(Amelia) #seems Amelia not installed in server
#missmap(train,col=c('yellow','black'),main='Titanic Train Data',legend=F)
check.missing<-function(x) return(paste0(round(sum(is.na(x))/length(x),4)*100,'%'))
data.frame(sapply(train,check.missing))
data.frame(sapply(test,check.missing))
#combine train/test data for pre-processing
train$Cat<-'train'
test$Cat<-'test'
test$Survived<-NA
full<-rbind(train,test)
#Embarked
full$Embarked[is.na(full$Embarked)]<-'S'
#Extract Title from Name
full$Title<-sapply(full$Name,function(x) strsplit(x,'[.,]')[[1]][2])
full$Title<-gsub(' ','',full$Title)
aggregate(Age~Title,full,median)
full$Title[full$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
full$Title[full$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
#Adding FamilySize
full$FamilySize<-full$Parch+full$SibSp+1
#Fare
#simply replace the one missing Fare data with median, due to skewed distribution of Fare
full$Fare[is.na(full$Fare)]<-median(full$Fare,na.rm=T)
#Age
#decision tree (regression) method to predict missing Age data
library(rpart)
fit.Age<-rpart(Age[!is.na(Age)]~Pclass+Title+Sex+SibSp+Parch+Fare,data=full[!is.na(full$Age),],method='anova')
full$Age[is.na(full$Age)]<-predict(fit.Age,full[is.na(full$Age),])
#Adding Mother
full$Mother<-0
full$Mother[full$Sex=='female' & full$Parch>0 & full$Age>18 & full$Title!='Miss']<-1
#Adding Child
full$Child<-0
full$Child[full$Parch>0 & full$Age<=18]<-1
#FamilyId2
Surname<-sapply(full$Name,function(x) strsplit(x,'[.,]')[[1]][1])
FamilyId<-paste0(full$FamilySize,Surname)
full$FamilyId<-factor(FamilyId)
Family<-data.frame(table(FamilyId))
SmallFamily<-Family$FamilyId[Family$Freq<=2]
FamilyId[FamilyId %in% SmallFamily]<-'Small'
full$FamilyId2<-factor(FamilyId)
#Exact Deck from Cabin number
full$Deck<-sapply(full$Cabin, function(x) strsplit(x,NULL)[[1]][1])
#deck.fit<-rpart(Deck~Pclass+Fare,data=full[!is.na(full$Deck),])
#full$Deck[is.na(full$Deck)]<-as.character(predict(deck.fit,full[is.na(full$Deck),],type='class'))
#full$Deck[is.na(full$Deck)]<-'UNK'
#Excat Position from Cabin number
full$CabinNum<-sapply(full$Cabin,function(x) strsplit(x,'[A-Z]')[[1]][2])
full$num<-as.numeric(full$CabinNum)
num<-full$num[!is.na(full$num)]
Pos<-kmeans(num,3)
full$CabinPos[!is.na(full$num)]<-Pos$cluster
full$CabinPos<-factor(full$CabinPos)
levels(full$CabinPos)<-c('Front','End','Middle')
full$num<-NULL
#side.train<-full[!is.na(full$Side),]
#side.test<-full[is.na(full$Side),]
#side.fit<-rpart(Side~FamilyId+FamilySize,side.train,method='class')
#full$Side[is.na(full$Side)]<-as.character(predict(side.fit,side.test,type='class'))
#factorize the categorical variables
full<-transform(full,
Pclass=factor(Pclass),
Sex=factor(Sex),
Embarked=factor(Embarked),
Title=factor(Title),
Mother=factor(Mother),
Child=factor(Child),
FamilyId2=factor(FamilyId2),
Deck=factor(Deck)
)
#split train/test data
train<-full[full$Cat=='train',]
test<-full[full$Cat=='test',]
train$Survived<-factor(train$Survived)
#randomForest method, (not support variables with too many levels, e.g. FamilyId here)
library(randomForest)
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=100,importance=T)
plot(fit.rf,main='randomForest error rate')
imp<-importance(fit.rf,type='1')
imp<-imp[order(imp),]
barplot(imp,las=1,cex.names=0.8,col='blue',horiz=T)
#cforest (conditional inference tree) method, (support variables with more levels and missing values, with unbiased prediction)
library(party)
fit<-cforest(Survived~FamilyId2+CabinPos+Deck+Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child+Deck,data=train,controls=cforest_unbiased(ntree=2000, mtry=3))
#write submission with cforest method
test$Survived<-predict(fit,test,OOB=TRUE,type='response')
submission<-test[,1:2]
write.csv(submission,'submission.csv',row.names=F)
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=1000,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=90,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=200,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=400,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=50,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=500,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=5000,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=100,importance=T)
plot(fit.rf,main='randomForest error rate')
fit.rf<-randomForest(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child,data=train,ntree=200,importance=T)
plot(fit.rf,main='randomForest error rate')
imp<-importance(fit.rf,type='1')
imp<-imp[order(imp),]
barplot(imp,las=1,cex.names=0.8,col='blue',horiz=T)
#cforest (conditional inference tree) method, (support variables with more levels and missing values, with unbiased prediction)
library(party)
fit<-cforest(Survived~FamilyId2+CabinPos+Deck+Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child+Deck,data=train,controls=cforest_unbiased(ntree=2000, mtry=3))
varimp(fit)
test$Survived<-predict(fit,test,OOB=TRUE,type='response')
submission<-test[,1:2]
write.csv(submission,'submission.csv',row.names=F)
fit<-cforest(Survived~FamilyId2+CabinPos+Deck+Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title+Mother+Child+Deck,data=train,controls=cforest_unbiased(ntree=2000, mtry=NULL))
varimp(fit)
library(gbm)
library(plyr)
library(ada)
install.packages("ada")
library(ada)
library(caret)
library(C50)
install.packages("C50")
library(C50)
boosted <- ada(survived ~ sex + pclass + age + fare, data = train)
View(test)
View(train)
boosted <- ada(Survived ~ sex + pclass + age + fare, data = train)
View(train)
boosted <- ada(Survived ~ Sex + Pclass + Age + Fare, data = train)
View(train)
boosted2 <- ada(survived ~ Sex.name + Pclass + Age + Fare + Family,
data = train, iter = 10000)
boosted2 <- ada(Survived ~ Sex.name + Pclass + Age + Fare + Family,
data = train, iter = 10000)
View(train)
View(Family)
View(full)
head(train)
str(train)
train=read.csv("train_clean.csv")  # 891 obs
setwd("~/Desktop/Titanic")
train=read.csv("train_clean.csv")  # 891 obs
train=read.csv("train_clean.csv")  # 891 obs
test=read.csv("test_clean.RData")   # 418 obs
test=read.csv("test_clean.csv")   # 418 obs
str(train)
boosted <- ada(Survived ~ Sex + Pclass + Age + Fare, data = train)
boosted <- ada(survived ~ sex + Pclass + Age + Fare, data = train)
head(train)
train=read.csv("train_clean.csv")  # 891 obs
test=read.csv("test_clean.csv")   # 418 obs
head(train)
###
### Create data model
###
str(train)
boosted <- ada(Survived ~ Sex + Pclass + Age + Fare, data = train)
boosted <- ada(survived ~ Sex + Pclass + Age + Fare, data = train)
boosted <- ada(survived ~ sex + pclass + age + fare, data = train)
load("~/Desktop/Titanic/mice.rda")
str(trainmice)
boosted <- ada(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Fare + Embarked + Title + FamilySize + FamilyID, data = trainmice)
boosted2 <- ada(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Fare + Embarked + Title + FamilySize + FamilyID,
data = train, iter = 10000)
boosted2 <- ada(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Fare + Embarked + Title + FamilySize + FamilyID,
data = trainmice, iter = 10000)
## Create C50 model
, data = trainmice)
data = trainmice)
crtree <- C5.0(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Fare + Embarked + Title + FamilySize + FamilyID,
data = trainmice)
train$survived_pred <- predict(boosted, train)
train$survived_pred <- predict(boosted, trainmice)
library(h2oEnsemble)  # Requires version >=0.1.8 of h2oEnsemble
localH2O <-  h2o.init(nthreads = -1)  # Start an H2O cluster with nthreads = num cores on your machine
h20.train<-as.h2o(trainmice)
train<-h20.train
h20.test<-as.h2o(testmice)
test<-h20.test
train['Ticket']=NULL
test['Ticket']=NULL
head(test)
test['Survived']=0
y <- c("Survived","PassengerId","Ticket")
x <- setdiff(names(h20.train), y)
y<-"Survived"
family <- "binomial"
#For binary classification, response should be a factor
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
search_criteria <- list(strategy = "RandomDiscrete",
max_runtime_secs = 1200)
nfolds <- 5
# GBM Hyperparamters
learn_rate_opt <- c(0.01, 0.03)
max_depth_opt <- c(3, 4, 5, 6, 9)
sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0)
col_sample_rate_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
hyper_params <- list(learn_rate = learn_rate_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate = col_sample_rate_opt)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = train,
ntrees = 100,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteria)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = train,
ntrees = 100,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteri)
search_criteri <- list(strategy = "RandomDiscrete",
max_runtime_secs = 1200)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = train,
ntrees = 100,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteri)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = train,
ntrees = 100,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
gbm_models <- lapply(gbm_grid@model_ids, function(model_id) h2o.getModel(model_id))
# RF Hyperparamters
mtries_opt <- 8:20
max_depth_opt <- c(5, 10, 15, 20, 25)
sample_rate_opt <- c(0.7, 0.8, 0.9, 1.0)
col_sample_rate_per_tree_opt <- c(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
hyper_params <- list(mtries = mtries_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt,
col_sample_rate_per_tree = col_sample_rate_per_tree_opt)
rf_grid <- h2o.grid("randomForest", x = x, y = y,
training_frame = train,
ntrees = 200,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteria)
rf_grid <- h2o.grid("randomForest", x = x, y = y,
training_frame = train,
ntrees = 200,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
hyper_params <- list(mtries = mtries_opt,
max_depth = max_depth_opt,
sample_rate = sample_rate_opt
)
rf_grid <- h2o.grid("randomForest", x = x, y = y,
training_frame = train,
ntrees = 200,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params
)
rf_models <- lapply(rf_grid@model_ids, function(model_id) h2o.getModel(model_id))
# Deeplearning Hyperparamters
activation_opt <- c("Rectifier", "RectifierWithDropout",
"Maxout", "MaxoutWithDropout")
hidden_opt <- list(c(10,10), c(20,15), c(50,50,50))
l1_opt <- c(0, 1e-3, 1e-5)
l2_opt <- c(0, 1e-3, 1e-5)
hyper_params <- list(activation = activation_opt,
hidden = hidden_opt,
l1 = l1_opt,
l2 = l2_opt)
dl_grid <- h2o.grid("deeplearning", x = x, y = y,
training_frame = train,
epochs = 15,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
search_criteria = search_criteria)
dl_models <- lapply(dl_grid@model_ids, function(model_id) h2o.getModel(model_id))
# GLM Hyperparamters
alpha_opt <- seq(0,1,0.1)
lambda_opt <- c(0,1e-7,1e-5,1e-3,1e-1)
hyper_params <- list(alpha = alpha_opt,
lambda = lambda_opt)
glm_grid <- h2o.grid("glm", x = x, y = y,
training_frame = train,
family = "binomial",
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,
dl_grid <- h2o.grid("deeplearning", x = x, y = y,
training_frame = train,
epochs = 20,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params)
)
dl_grid <- h2o.grid("deeplearning", x = x, y = y,
training_frame = train,
epochs = 20,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params)
gbm_grid <- h2o.grid("gbm", x = x, y = y,
gbm_grid <- h2o.grid("gbm", x = x, y = y,
training_frame = train,
ntrees = 100,
seed = 1,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE,
hyper_params = hyper_params,search_criteria=search_criteri
)
)
